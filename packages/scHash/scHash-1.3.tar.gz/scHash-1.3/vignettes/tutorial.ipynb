{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "043268a0-8b52-4b28-81f7-7faee4a63029",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scHash\n",
    "import anndata as ad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c97c2dc-b012-4eeb-a949-e00cf361a16d",
   "metadata": {},
   "source": [
    "## Pancreas Dataset\n",
    "We demonstrate how scHash encode multiple datasets into hash codes for six public avaialble Pancreas datasets.\n",
    "\n",
    "The raw data for first five datasets can be obtained from [Harmony](https://github.com/immunogenomics/harmony2019/tree/master/data/figure5).\n",
    "\n",
    "The sixth Pancreas dataset is public available at GSE83139.\n",
    "\n",
    "We compiled the six datasets into one AnnData object for easy demonstration. The processed data can be downloaded [here](https://drive.google.com/file/d/1shc4OYIbq2FwbyGUaYuzizuvzW-giSTs/view?usp=share_link)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317a0be4-3a6d-4cec-9038-573aa886e17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../share_data/Pancreas_Wang/fivepancreas_wang_raw.h5ad'\n",
    "\n",
    "# set up datamodule\n",
    "# This anndata object is packed with 6 pancreas dataset. We take one of them to be a test dataset here.  \n",
    "query = 'wang'\n",
    "full = ad.read_h5ad(data_dir)\n",
    "train = full[full.obs.dataset!=query]\n",
    "test = full[full.obs.dataset==query]\n",
    "\n",
    "datamodule = scHash.setup_training_data(train_data = train,cell_type_key = 'cell_type', batch_key = 'dataset')\n",
    "\n",
    "# set the query data\n",
    "# this can be also set after train\n",
    "datamodule.setup_test_data(test)\n",
    "\n",
    "########### consider write into a function again\n",
    "# Init ModelCheckpoint callback\n",
    "checkpointPath = '../checkpoint/'\n",
    "\n",
    "# Init the model and Train\n",
    "model = scHash.scHashModel(datamodule)\n",
    "trainer, best_model_path = scHash.training(model = model, datamodule = datamodule, checkpointPath = checkpointPath, max_epochs = 100)\n",
    "\n",
    "# Test the best model\n",
    "scHash.testing(trainer, model, best_model_path, datamodule)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b7dc17a-3125-434d-bf72-7aca40588d2e",
   "metadata": {},
   "source": [
    "# Atlas Dataset\n",
    "\n",
    "Here is an demonstration on atlas level dataset. We demonstrate the the atlas level annotation with the dataset Tabula Senis Muris and it can be download [here](https://figshare.com/projects/Tabula_Muris_Senis/64982). We followed scArches' preprocess pipeline and the preprocessed data can be downloaded through [here]( https://drive.google.com/file/d/1lfDu-TGsUvHrmXoSWkj0tptvWNYFgs2x/view?usp=share_link). The dataset contains 356213 cells with 5000 highly variable genes with cell type, method, and tissue annotations. \n",
    "\n",
    "The steps aredata the same for atlas level datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f2c91a-8c47-442f-a888-a796c4c0e766",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../../share_data/Tabula_Muris_Senis(TM)/tabula_senis_normalized_all_hvg.h5ad'\n",
    "\n",
    "# This data contains both the reference and query source\n",
    "data = ad.read_h5ad(data_dir)\n",
    "\n",
    "# random split to get query indices\n",
    "# import random \n",
    "from sklearn.model_selection import train_test_split\n",
    "reference_indicies, query_indicies = train_test_split(list(range(data.shape[0])), train_size=0.8, stratify=data.obs.cell_ontology_class,random_state=42)\n",
    "\n",
    "train = data[reference_indicies]\n",
    "test = data[query_indicies]\n",
    "datamodule = scHash.setup_training_data(train_data = train,cell_type_key = 'cell_ontology_class')\n",
    "\n",
    "# set the query data\n",
    "# this can be also set after train\n",
    "datamodule.setup_test_data(test)\n",
    "\n",
    "########### consider write into a function again\n",
    "# Init ModelCheckpoint callback\n",
    "checkpointPath = '../checkpoint/'\n",
    "\n",
    "# Init the model and Train\n",
    "model = scHash.scHashModel(datamodule, bit = 256)\n",
    "trainer, best_model_path = scHash.training(model = model, datamodule = datamodule, checkpointPath = checkpointPath, max_epochs = 100)\n",
    "\n",
    "# Test the best model\n",
    "scHash.testing(trainer, model, best_model_path, datamodule)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jupyter_py3",
   "language": "python",
   "name": "jupyter_py3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
