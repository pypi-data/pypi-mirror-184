Metadata-Version: 2.1
Name: analogvnn
Version: 1.0.0rc0
Summary: A fully modular framework for modeling and optimizing analog/photonic neural networks
Keywords: deep-learning,analog,photonics,neural-network,framework,pytorch
Author-email: Vivswan Shah <vivswanshah@pitt.edu>
Maintainer-email: Vivswan Shah <vivswanshah@pitt.edu>
Requires-Python: >=3.7
Description-Content-Type: text/markdown
Classifier: Development Status :: 5 - Production/Stable
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: Topic :: Software Development :: Build Tools
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: License :: OSI Approved :: GNU General Public License v3 (GPLv3)
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.7
Classifier: Programming Language :: Python :: 3.8
Classifier: Programming Language :: Python :: 3.9
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3 :: Only
Requires-Dist: dataclasses
Requires-Dist: scipy
Requires-Dist: numpy>=1.16.5
Requires-Dist: networkx
Requires-Dist: importlib_metadata
Requires-Dist: analogvnn[research,dev,doc] ; extra == "all"
Requires-Dist: setuptools>=61.0.0 ; extra == "dev"
Requires-Dist: build ; extra == "dev"
Requires-Dist: twine ; extra == "dev"
Requires-Dist: johnnydep ; extra == "dev"
Requires-Dist: sphinx>=4.2.0 ; extra == "doc"
Requires-Dist: rst-to-myst[sphinx] ; extra == "doc"
Requires-Dist: furo ; extra == "doc"
Requires-Dist: myst_parser ; extra == "doc"
Requires-Dist: sphinx-autoapi ; extra == "doc"
Requires-Dist: sphinxext-opengraph ; extra == "doc"
Requires-Dist: sphinx-inline-tabs ; extra == "doc"
Requires-Dist: sphinx-notfound-page ; extra == "doc"
Requires-Dist: sphinx-copybutton ; extra == "doc"
Requires-Dist: sphinxcontrib-katex ; extra == "doc"
Requires-Dist: tensorflow ; extra == "research"
Requires-Dist: tensorboard ; extra == "research"
Requires-Dist: torchviz ; extra == "research"
Requires-Dist: matplotlib ; extra == "research"
Requires-Dist: tabulate ; extra == "research"
Requires-Dist: pillow ; extra == "research"
Requires-Dist: graphviz ; extra == "research"
Requires-Dist: seaborn ; extra == "research"
Requires-Dist: natsort ; extra == "research"
Project-URL: Author, https://vivswan.github.io/
Project-URL: Bug Reports, https://github.com/Vivswan/AnalogVNN/issues
Project-URL: Documentation, https://analogvnn.readthedocs.io/en/latest/
Project-URL: Homepage, https://github.com/Vivswan/AnalogVNN
Project-URL: Say Thanks!, https://github.com/Vivswan
Project-URL: Source, https://github.com/Vivswan/AnalogVNN
Provides-Extra: all
Provides-Extra: dev
Provides-Extra: doc
Provides-Extra: research

# AnalogVNN

AnalogVNN Paper: [https://arxiv.org/abs/2210.10048](https://arxiv.org/abs/2210.10048)

Documentation: [https://analogvnn.readthedocs.io/](https://analogvnn.readthedocs.io/)

Cite: Vivswan Shah, and Nathan Youngblood. "AnalogVNN: A fully modular framework for modeling and optimizing photonic neural
networks." *arXiv preprint arXiv:2210.10048 (2022)*.

Installation:
- Install [PyTorch](https://pytorch.org/)
- Install AnanlogVNN using [pip](https://pypi.org/project/analogvnn/)
```bash
pip install analogvnn
```

AnalogVNN is a simulation framework built on PyTorch which can simulate the effects of
optoelectronic noise, limited precision, and signal normalization present in photonic
neural network accelerators. We use this framework to train and optimize linear and
convolutional neural networks with up to 9 layers and ~1.7 million parameters, while
gaining insights into how normalization, activation function, reduced precision, and
noise influence accuracy in analog photonic neural networks. By following the same layer
structure design present in PyTorch, the AnalogVNN framework allows users to convert most
digital neural network models to their analog counterparts with just a few lines of code,
taking full advantage of the open-source optimization, deep learning, and GPU acceleration
libraries available through PyTorch.

