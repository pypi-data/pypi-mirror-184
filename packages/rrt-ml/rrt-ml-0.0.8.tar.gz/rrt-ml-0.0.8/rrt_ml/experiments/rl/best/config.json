{
    "action_l2_regularization":	0.1,
    "activ":	"gelu",
    "actor_lr":	0.001,
    "actor_weight_decay":	0.0,
    "agent_name":	"best",
    "annotationdict":	{
        "action_l2_regularization":	"l2 penalty for action norm",
        "action_noise":	"maximum std of action noise",
        "activ":	"activation to use for hidden layers in networks",
        "actor_lr":	"actor learning rate",
        "actor_weight_decay":	"weight decay to apply to actor",
        "batch_size":	"batch size for training the actors/critics",
        "clip_target_range":	"q/value targets are clipped to this range",
        "critic_lr":	"critic learning rate",
        "critic_weight_decay":	"weight decay to apply to critic",
        "curiosity_beta":	"beta to use for curiosity_alpha module",
        "device":	"torch device (cpu or gpu)",
        "dg_score_multiplier":	"if using instrinsic goals, score multiplier for goal candidates that are in DG distribution",
        "direct_overshoots":	"if using overshooting, should it be directed in a straight line?",
        "eexplore":	"how often to do completely random exploration (overrides action noise)",
        "entropy_coef":	"Entropy regularization coefficient for SAC",
        "future_warm_up":	"minimum steps in replay buffer needed to stop doing ONLY future sampling",
        "gamma":	"discount factor",
        "go_eexplore":	"epsilon exploration bonus from each point of go explore, when using intrinsic curiosity",
        "go_reset_percent":	"probability to reset episode early for each point of go explore, when using intrinsic curiosity",
        "grad_norm_clipping":	"gradient norm clipping",
        "grad_value_clipping":	"gradient value clipping",
        "her":	"strategy to use for hindsight experience replay",
        "initial_cutoff":	"initial (and minimum) cutoff for intrinsic goal curiosity",
        "initial_explore":	"steps that actor acts randomly for at beginning of training",
        "log_every":	"how often to log things",
        "n_step_returns":	"if using n-step returns, how many steps?",
        "num_envs":	"number of parallel envs to run",
        "num_eval_envs":	"number of parallel eval envs to run",
        "optimize_every":	"how often optimize is called, in terms of environment steps",
        "overshoot_goal_percent":	"if using instrinsic FIRST VISIT goals, should goal be overshot on success?",
        "policy_opt_noise":	"how much policy noise to add to actor optimization",
        "prioritized_mode":	"buffer prioritization strategy",
        "replay_size":	"maximum size of replay buffer",
        "save_replay_buf":	"save replay buffer checkpoint during training?",
        "seed":	"random seed",
        "sigma_l2_regularization":	"l2 regularization on sigma critics log variance",
        "slot_based_state":	"if state is organized by slot; i.e., [batch_size, num_slots, slot_feats]",
        "sparse_reward_shaping":	"coefficient of euclidean distance reward shaping in sparse goal envs",
        "target_network_update_frac":	"polyak averaging coefficient for target networks",
        "target_network_update_freq":	"how often to update target networks; NOTE: TD3 uses this too!",
        "td3_delay":	"how often the actor is trained, in terms of critic training steps, in td3",
        "td3_noise":	"noise added to next step actions in td3",
        "td3_noise_clip":	"amount to which next step noise in td3 is clipped",
        "use_actor_target":	"if true, use actor target network to act in the environment",
        "varied_action_noise":	"if true, action noise for each env in vecenv is interpolated between 0 and action noise",
        "warm_up":	"minimum steps in replay buffer needed to optimize"
    },
    "batch_size":	2048,
    "clip_target_range":	[
        -50.0,
        0.0
    ],
    "critic_lr":	0.001,
    "critic_weight_decay":	0.0,
    "curiosity_beta":	-3.0,
    "cutoff_success_threshold":	[
        0.3,
        0.7
    ],
    "device":	"cpu",
    "dg_score_multiplier":	1.0,
    "direct_overshoots":	false,
    "eexplore":	0.1,
    "entropy_coef":	0.2,
    "env_steps":	0,
    "first_visit_done":	false,
    "first_visit_succ":	true,
    "future_warm_up":	25000,
    "gamma":	0.98,
    "go_eexplore":	0.1,
    "go_reset_percent":	0.0,
    "grad_norm_clipping":	-1.0,
    "grad_value_clipping":	5.0,
    "her":	"rfaab_1_4_3_1_1",
    "initial_cutoff":	-3,
    "initial_explore":	5000,
    "layers":	[
        512,
        512,
        512
    ],
    "log_every":	5000,
    "module_action_noise":	"{'self': <mrl.modules.action_noise.ContinuousActionNoise object at 0x000001AE772EB550>, 'random_process_cls': <class 'mrl.utils.random_process.GaussianProcess'>, 'args': (), 'kwargs': {'std': <mrl.utils.schedule.ConstantSchedule object at 0x000001AE772EAB00>}}",
    "module_actor":	"{'self': <mrl.modules.model.PytorchModel object at 0x000001AE772EB760>, 'name': 'actor', 'model_fn': <function RL._setup_agent.<locals>.<lambda> at 0x000001AE757DC5E0>}",
    "module_ag_curiosity":	"{'self': <mrl.modules.curiosity.DensityAchievedGoalCuriosity object at 0x000001AE772EB490>, 'num_sampled_ags': 100, 'max_steps': 100, 'keep_dg_percent': -0.1, 'randomize': False, 'use_qcutoff': True}",
    "module_ag_kde":	"{'self': <mrl.modules.density.RawKernelDensity object at 0x000001AE772EAA10>, 'item': 'ag', 'optimize_every': 1, 'samples': 10000, 'kernel': 'gaussian', 'bandwidth': 0.1, 'normalize': True, 'log_entropy': True, 'tag': '', 'buffer_name': 'replay_buffer'}",
    "module_ag_kde_tophat":	"{'self': <mrl.modules.density.RawKernelDensity object at 0x000001AE772EAAA0>, 'item': 'ag', 'optimize_every': 100, 'samples': 10000, 'kernel': 'tophat', 'bandwidth': 0.2, 'normalize': True, 'log_entropy': False, 'tag': '_tophat', 'buffer_name': 'replay_buffer'}",
    "module_algorithm":	"{'self': <mrl.algorithms.continuous_off_policy.DDPG object at 0x000001AE772EB8B0>}",
    "module_critic":	"{'self': <mrl.modules.model.PytorchModel object at 0x000001AE757C4E50>, 'name': 'critic', 'model_fn': <function RL._setup_agent.<locals>.<lambda> at 0x000001AE757DCEE0>}",
    "module_env":	"{'self': <mrl.modules.env.EnvModule object at 0x000001AE756D9AB0>, 'num_envs': 1, 'name': None, 'env': <function RL._setup_agent.<locals>.train_env_fn at 0x000001AE757BEB00>, 'episode_life': True, 'seed': None}",
    "module_eval":	"{'self': <mrl.modules.eval.EpisodicEval object at 0x000001AE772EAA40>}",
    "module_eval_env":	"{'self': <mrl.modules.env.EnvModule object at 0x000001AE772EB8E0>, 'num_envs': 1, 'name': 'eval_env', 'env': <function RL._setup_agent.<locals>.val_env_fn at 0x000001AE757DC310>, 'episode_life': True, 'seed': None}",
    "module_goal_reward":	"{'self': <mrl.modules.goal_reward.GoalEnvReward object at 0x000001AE757C5990>}",
    "module_logger":	"{'self': <mrl.modules.logging.Logger object at 0x000001AE772EAAD0>, 'average_every': 100}",
    "module_policy":	"{'self': <mrl.algorithms.continuous_off_policy.ActorPolicy object at 0x000001AE772EAEC0>}",
    "module_replay_buffer":	"{'self': <mrl.replays.online_her_buffer.OnlineHERBuffer object at 0x000001AE772EA7D0>, 'module_name': 'replay_buffer'}",
    "module_state_normalizer":	"{'self': <mrl.modules.normalizer.Normalizer object at 0x000001AE772EA800>, 'normalizer': <mrl.modules.normalizer.MeanStdNormalizer object at 0x000001AE772EAB90>}",
    "module_train":	"{'self': <mrl.modules.train.StandardTrain object at 0x000001AE772EA8C0>}",
    "n_step_returns":	1,
    "num_envs":	1,
    "num_eval_envs":	1,
    "opt_steps":	0,
    "optimize_every":	1,
    "overshoot_goal_percent":	0.0,
    "parent_folder":	"C:\\Users\\jpuer\\Documents\\GitHub\\RRT-ML\\rrt_ml\\experiments\\rl",
    "policy_opt_noise":	0.0,
    "prioritized_mode":	"none",
    "replay_size":	1000000,
    "save_replay_buf":	false,
    "seed":	0,
    "sigma_l2_regularization":	0.0,
    "slot_based_state":	false,
    "sparse_reward_shaping":	0.0,
    "target_network_update_frac":	0.05,
    "target_network_update_freq":	40,
    "td3_delay":	2,
    "td3_noise":	0.1,
    "td3_noise_clip":	0.3,
    "train_timestep":	1000000,
    "use_actor_target":	false,
    "varied_action_noise":	false,
    "warm_up":	2500
}